{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a work in progress.\n",
    "In sequence:\n",
    "1. Crawl your spiders, and store the output data as json, in a specific folder.\n",
    "2. Step 1 (individual json): Map each org_id with org_sort. The map reference is in org_mapping.json\n",
    "3. Step 2 (individual json): Create UUID & Sort data according to; division_sort_order, person_sort_order. \n",
    "- A new key is introduced: 'person_sort'. This fixes the incorrect 'person_sort_order'.\n",
    "4. Step 3 (all json): Compile all data.\n",
    "- Removed 'person_sort_order'.\n",
    "5. Step 4 (all json): Re-sort the compiled.json according to; the org_id, division_sort, person_sort.\n",
    "- The final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mapping org_id with org_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted output1/v2_petra_org_mapped.json\n",
      "Deleted output1/v2_mof_org_mapped.json\n",
      "Deleted output1/v2_kkdw_anggota_org_mapped.json\n",
      "Deleted output1/v2_kpkm_org_mapped.json\n",
      "Deleted output1/v2_kln_org_mapped.json\n",
      "Processed v2_mof.json. Saved to output1/v2_mof_org_mapped.json\n",
      "Processed v2_kkdw_anggota.json. Saved to output1/v2_kkdw_anggota_org_mapped.json\n",
      "Processed v2_kln.json. Saved to output1/v2_kln_org_mapped.json\n",
      "Processed v2_kpkm.json. Saved to output1/v2_kpkm_org_mapped.json\n",
      "Processed v2_petra.json. Saved to output1/v2_petra_org_mapped.json\n",
      "All files processed and saved in output1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_org_mapping(mapping_file):\n",
    "    with open(mapping_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def load_spider_output(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_updated_data(data, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def map_org_sort(spider_output, org_mapping):\n",
    "    for entry in spider_output:\n",
    "        org_id = entry.get('org_id')\n",
    "        \n",
    "        if not org_id:\n",
    "            print(f\"Warning: 'org_id' missing in entry: {entry}\")\n",
    "            continue  #skip this entry if org_id is missing\n",
    "        \n",
    "        # Map org_sort if org_id exists in the mapping, otherwise log a warning\n",
    "        if org_id in org_mapping:\n",
    "            entry['org_sort'] = org_mapping[org_id]\n",
    "        else:\n",
    "            print(f\"Warning: 'org_id' {org_id} not found in org_mapping.\")\n",
    "\n",
    "    return spider_output\n",
    "\n",
    "def clean_output_directory(output_dir):\n",
    "    \"\"\"Function to clean the output directory by deleting existing files\"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        for filename in os.listdir(output_dir):\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted {file_path}\")\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)  #create the directory if it doesn't exist\n",
    "\n",
    "def process_all_files(input_dir, output_dir, org_mapping): #main\n",
    "    clean_output_directory(output_dir)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            input_file_path = os.path.join(input_dir, filename)\n",
    "            \n",
    "            output_file_name = f\"{os.path.splitext(filename)[0]}_org_mapped.json\"\n",
    "            output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "            spider_output = load_spider_output(input_file_path)\n",
    "\n",
    "            updated_data = map_org_sort(spider_output, org_mapping)\n",
    "\n",
    "            save_updated_data(updated_data, output_file_path)\n",
    "\n",
    "            print(f\"Processed {filename}. Saved to {output_file_path}\")\n",
    "\n",
    "def main():\n",
    "    input_dir = 'input'  # Folder containing the spider output JSON files\n",
    "    output_dir = 'output1'  # Folder where the updated JSON files will be saved\n",
    "    org_mapping_file = 'org_mapping.json'  # File containing the org_id to org_sort mapping\n",
    "\n",
    "    # Load the org_id to org_sort mapping\n",
    "    org_mapping = load_org_mapping(org_mapping_file)\n",
    "\n",
    "    process_all_files(input_dir, output_dir, org_mapping)\n",
    "\n",
    "    print(f\"All files processed and saved in {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create UUID & Sort according to : division_sort_order, person_sort_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted output2/v2_kpkm_org_mapped_sorted.json\n",
      "Deleted output2/v2_mof_org_mapped_sorted.json\n",
      "Deleted output2/v2_kkdw_anggota_org_mapped_sorted.json\n",
      "Deleted output2/v2_kln_org_mapped_sorted.json\n",
      "Deleted output2/v2_petra_org_mapped_sorted.json\n",
      "Processed v2_petra_org_mapped.json and saved to v2_petra_org_mapped_sorted.json\n",
      "Processed v2_mof_org_mapped.json and saved to v2_mof_org_mapped_sorted.json\n",
      "Processed v2_kkdw_anggota_org_mapped.json and saved to v2_kkdw_anggota_org_mapped_sorted.json\n",
      "Processed v2_kpkm_org_mapped.json and saved to v2_kpkm_org_mapped_sorted.json\n",
      "Processed v2_kln_org_mapped.json and saved to v2_kln_org_mapped_sorted.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def write_json_file(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('[\\n')\n",
    "        for i, obj in enumerate(data):\n",
    "            if i > 0:\n",
    "                file.write(',\\n')  #add a comma and newline between objects\n",
    "            file.write(json.dumps(obj, separators=(',', ':')))  #write each object compactly\n",
    "        file.write('\\n]')\n",
    "\n",
    "def generate_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# def process_data(data): #Option 1: the person_sort does not reset for each division\n",
    "#     sorted_data = sorted(data, key=lambda x: (x['division_sort'], x['person_sort_order']))\n",
    "#     processed_data = []\n",
    "#     for idx, entry in enumerate(sorted_data):\n",
    "#         new_entry = {'id': generate_uuid()}\n",
    "#         new_entry.update({'person_sort': idx + 1})\n",
    "#         new_entry.update(entry)\n",
    "#         processed_data.append(new_entry)\n",
    "#     return processed_data\n",
    "\n",
    "def process_data(data): #Option 2: the person_sort will reset for each division\n",
    "    sorted_data = sorted(data, key=lambda x: (x['division_sort'], x['person_sort_order'])) #sort the data based on 'division_sort' and 'person_sort_order'\n",
    "    processed_data = []\n",
    "    current_division = None\n",
    "    person_sort_counter = 0\n",
    "\n",
    "    for entry in sorted_data:\n",
    "        #check if the division has changed and reset the person_sort_counter\n",
    "        if entry['division_sort'] != current_division:\n",
    "            current_division = entry['division_sort']\n",
    "            person_sort_counter = 1\n",
    "        else:\n",
    "            person_sort_counter += 1\n",
    "\n",
    "        #new_entry = {'id': generate_uuid()}  # Place 'id' at the beginning\n",
    "        #new_entry.update({'person_sort': person_sort_counter})  # Reset person_sort for each division\n",
    "        new_entry = entry\n",
    "        new_entry.update({'person_sort': person_sort_counter})  # Reset person_sort for each division\n",
    "        new_entry.update(entry)  # Add the rest of the original data\n",
    "        processed_data.append(new_entry)\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def clean_output_directory(output_dir):\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.endswith(\"_sorted.json\"):\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted {file_path}\")\n",
    "\n",
    "#main function to process all JSON files in a directory\n",
    "def process_all_files(input_dir, output_dir=None):\n",
    "    #if no output directory is specified, use the input directory\n",
    "    if output_dir is None:\n",
    "        output_dir = input_dir\n",
    "\n",
    "    #ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    #clean the output directory by removing existing \"_sorted.json\" files\n",
    "    clean_output_directory(output_dir)\n",
    "\n",
    "    # Iterate over all files in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            input_file_path = os.path.join(input_dir, filename)\n",
    "            \n",
    "            data = read_json_file(input_file_path)\n",
    "            \n",
    "            processed_data = process_data(data)\n",
    "            \n",
    "            output_file_name = f\"{os.path.splitext(filename)[0]}_sorted.json\"\n",
    "            output_file_path = os.path.join(output_dir, output_file_name)\n",
    "            \n",
    "            write_json_file(output_file_path, processed_data)\n",
    "            \n",
    "            print(f\"Processed {filename} and saved to {output_file_name}\")\n",
    "\n",
    "input_dir = 'output1'  # where you store the input files\n",
    "output_dir = 'output2'  # will default to input directory if not provided\n",
    "\n",
    "process_all_files(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compile all data\n",
    "- check whether the required keys are there (based on the discussed json structure)\n",
    "- remove 'person_sort_order'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing compiled2.json\n",
      "Compiled 5 JSON files into compiled2.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "\n",
    "input_folder = 'output2/'\n",
    "output_file = 'compiled2.json'\n",
    "\n",
    "required_keys = [\n",
    "    #\"id\", \n",
    "    \"org_sort\", \"org_id\", \"org_name\", \"org_type\", \"division_sort\",\n",
    "    \"division_name\", \"unit_name\", \"person_sort\", \"person_name\",\n",
    "    \"person_position\", \"person_email\", \"person_phone\", \"person_fax\", \"parent_org_id\"\n",
    "]\n",
    "\n",
    "def ensure_required_keys(entry, json_file):\n",
    "    # Add a UUID for the 'id' field if missing\n",
    "    # if 'id' not in entry:\n",
    "    #     entry['id'] = str(uuid.uuid4())\n",
    "    #     print(f\"Missing 'id' in {json_file}, assigning UUID.\")\n",
    "\n",
    "    for key in required_keys:\n",
    "        if key not in entry:\n",
    "            print(f\"Missing '{key}' in {json_file}, setting default value.\")\n",
    "            if key == \"parent_org_id\":\n",
    "                entry[key] = []  # Ensure 'parent_org_id' is always a list\n",
    "            else:\n",
    "                entry[key] = None  # Default value for missing fields\n",
    "\n",
    "    return entry\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "    print(f\"Deleted existing {output_file}\")\n",
    "\n",
    "compiled_data = []\n",
    "\n",
    "if os.path.exists(input_folder):\n",
    "    json_files = glob.glob(os.path.join(input_folder, '*.json')) #loop\n",
    "\n",
    "    #read each file and append its content to the compiled_data list\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            for entry in data:\n",
    "                #remove 'person_sort_order' if it exists in any of the entries\n",
    "                if 'person_sort_order' in entry:\n",
    "                    del entry['person_sort_order']\n",
    "\n",
    "                #ensure each entry has all required keys, and log missing keys\n",
    "                entry = ensure_required_keys(entry, json_file)\n",
    "\n",
    "                compiled_data.extend([entry])  #combine all entries from each JSON\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(compiled_data, outfile, indent=4)\n",
    "\n",
    "    print(f\"Compiled {len(json_files)} JSON files into {output_file}\")\n",
    "else:\n",
    "    print(f\"The folder {input_folder} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Re-sort the compiled.json - the org_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted data and saved to compiled_sorted_v2.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "compiled_file = 'compiled2.json'\n",
    "output_file = 'compiled_sorted_v2.json'\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def write_json_file(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "#sort the data based on 'org_sort', 'division_sort', and 'person_sort'\n",
    "def sort_data(data):\n",
    "    return sorted(data, key=lambda x: (x['org_sort'], x['division_sort'], x['person_sort']))\n",
    "\n",
    "if os.path.exists(compiled_file):\n",
    "    compiled_data = read_json_file(compiled_file)\n",
    "\n",
    "    sorted_data = sort_data(compiled_data)\n",
    "\n",
    "    write_json_file(output_file, sorted_data)\n",
    "\n",
    "    print(f\"Sorted data and saved to {output_file}\")\n",
    "else:\n",
    "    print(f\"The file {compiled_file} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
