name: Run Pipeline

on:
  push:
    branches: [ "feat/actions" ]
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  run-pipeline:
    runs-on: ubuntu-latest

    env:
      DIRECTORY_SRC: directory_scraper/src

    steps:
      - name: Check out the code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: '3.11'

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt', 'directory_scraper/setup.py') }}
          restore-keys: |
             ${{ runner.os }}-pip-

      - name: Test connectivity to target site with timeout
        run: |
          curl -I --max-time 10 https://www.mof.gov.my/portal/ms/hubungi/direktori || echo "Could not connect to site"

      - name: Install package setup.py in editable mode
        run: pip install -e directory_scraper/

      - name: Run spiders and store raw data in data/spiders_output
        run: python $DIRECTORY_SRC/data_processing/run_spiders.py mof

      - name: Upload spider output as artifact
        uses: actions/upload-artifact@v3
        with:
          name: spider-output
          path: ${{ github.workspace }}/directory_scraper/src/data_processing/data/spiders_output
          retention-days: 5

      - name: Upload run_spiders.log as an artifact
        uses: actions/upload-artifact@v3
        with:
          name: run_spiders-log
          path: directory_scraper/logs/run_spiders.log
          retention-days: 5

      - name: Upload run_spiders.log as an artifact
        uses: actions/upload-artifact@v3
        with:
          name: run_spiders-log
          path: directory_scraper/logs/run_spiders_custom.log
          retention-days: 5
