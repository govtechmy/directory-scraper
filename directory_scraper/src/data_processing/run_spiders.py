import json
import os
import shutil
import logging
from datetime import datetime
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from scrapy.spiderloader import SpiderLoader
import re

# Logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # logging level for custom logger

LOG_DIR = os.path.join(os.path.dirname(__file__), '../..', 'logs') 
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)
LOG_FILE_NAME = 'run_spiders.log'
LOG_FILE_PATH = os.path.join(LOG_DIR, LOG_FILE_NAME)

file_handler = logging.FileHandler(LOG_FILE_PATH)
file_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)

logger.addHandler(file_handler)
logger.propagate = True  # Propagate logs to Scrapy's root logger

# Global success and fail counters
success_count = 0
fail_count = 0
success_spiders = []
fail_spiders = []

# Define specific spiders if to use run_specific_spiders(), else run_all_spiders()
SPECIFIC_SPIDERS = [
    # "jpm",
    # "mof",
    # "rurallink",
    # "petra",
     "mot",
    # "kpkm",
    # "ekonomi",
    # "kpkt",
    # "kln",
    # "kkr",
    # "moha",
    # "miti",
    # "mod",
    # "mosti",
    # "kpwkm",
    # "nres",
    # "kuskop",
    # "kpt",
    # "motac",
    # "komunikasi",
    # "moe",
    # "kpn",
    # "kbs",
    # "jwp",
    # "kpdn",
    # "kpk",
    # "digital",
    # "moh",
    # "mohr"
]

def filter_custom_logs(LOG_FILE_PATH=LOG_FILE_PATH):
    """
    This function filters out custom logs from the main log file (LOG_FILE_PATH),
    and writes only the entries generated by the custom logger (identified using the __name__ attribute) into a new log file, 
    excluding Scrapy's internal logs (identified using regex pattern) (e.g., [scrapy.core.engine], [scrapy-playwright], etc)

    Reason: Scrapy manages the entire logging system for spiders, making it hard to isolate custom logs from Scrapy's logs during runtime. 
    This function filters the logs post-run, allowing us to separate and extract only the custom logs based on the module's name (__name__).

    Arguments:
    LOG_FILE_PATH : str (optional)
    The path to the main log file to filter (defaults to LOG_FILE_PATH).
    """
    input_log = LOG_FILE_PATH
    output_log = LOG_FILE_PATH.replace('.log', '_custom.log')
    
    custom_logger_name = f"[{__name__}]"
    scrapy_pattern = re.compile(r"\[scrapy(?:\..*?)?\]")
    non_scrapy_pattern = re.compile(r", (DEBUG|INFO|WARNING|ERROR|CRITICAL) - .* - ")

    try:
        with open(input_log, "r") as infile:
            with open(output_log, "w") as outfile:
                for line in infile:
                    if scrapy_pattern.search(line):
                        continue
                    elif custom_logger_name in line:
                        outfile.write(line)
                    elif non_scrapy_pattern.search(line) and "scrapy" not in line:
                        outfile.write(line)
                    elif not scrapy_pattern.search(line) and "scrapy" not in line:
                        outfile.write(line)
                        
        print(f"Filtered custom logs have been written to {output_log}")
    except FileNotFoundError:
        print(f"Error: The file {input_log} does not exist.")
    except Exception as e:
        print(f"An error occurred: {e}")

def setup_output_folder(folder_path, spider_names):
    """
    - Prepares the output folder for storing spider results.
    - Creates the output folder if it doesn't exist.
    - Deletes any files or directories in the folder that match the spider names.

    Args:
    folder_path (str): The path to the folder where spider output will be stored.
    spider_names (list): List of spider names whose related files/folders need to be removed.
    """
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
    else:
        for f in os.listdir(folder_path):
            file_path = os.path.join(folder_path, f)
            file_name, file_ext = os.path.splitext(f)
            try:
                if file_name in spider_names and os.path.isfile(file_path):
                    os.remove(file_path)
                    logger.info(f"Deleted file: {file_path}")
                elif file_name in spider_names and os.path.isdir(file_path):
                    shutil.rmtree(file_path)
                    logger.info(f"Deleted directory: {file_path}")
            except Exception as e:
                logger.warning(f"Failed to delete {file_path}. Reason: {e}")
        logger.info(f"Done.. deleted relevant files in {folder_path}")

def backup_spider_outputs(output_folder, spider_names, backup_folder, max_backups=5):
    """
    Backs up existing spider output files to a backup folder.
    Creates timestamped backups and removes older backups if exceeding max_backups.

    Args:
    output_folder (str): Path to the folder containing the spider output files.
    spider_names (list): List of spider names whose output should be backed up.
    backup_folder (str): Path to the folder where backups will be stored.
    max_backups (int): Maximum number of backups to retain for each spider outputs. 
    """

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if not os.path.exists(backup_folder):
        os.makedirs(backup_folder)

    for spider_name in spider_names:
        spider_output_file = os.path.join(output_folder, f"{spider_name}.json")
        if os.path.exists(spider_output_file):
            spider_backup_folder = os.path.join(backup_folder, spider_name)
            if not os.path.exists(spider_backup_folder):
                os.makedirs(spider_backup_folder)
            
            backup_file_path = os.path.join(spider_backup_folder, f"{spider_name}_{timestamp}.json")
            try:
                shutil.copy(spider_output_file, backup_file_path)
                logger.info(f"Backed up {spider_output_file} to {backup_file_path}")
            except Exception as e:
                logger.error(f"Error backing up {spider_output_file}: {e}")

            backups = sorted(os.listdir(spider_backup_folder), reverse=True)
            if len(backups) > max_backups:
                backups_to_remove = backups[max_backups:]
                for backup in backups_to_remove:
                    os.remove(os.path.join(spider_backup_folder, backup))
                    logger.info(f"Removed old backup: {backup}")

def setup_crawler(spiders):
    """
    Sets up and configures the Scrapy crawler with custom settings.
    Configures retry attempts, timeouts, logging, and integrates custom pipelines.

    Args:
    spiders (list): List of spiders for crawling.

    Returns:
    CrawlerProcess: A Scrapy CrawlerProcess instance with configured settings.
    """
    settings = get_project_settings()
    settings.set('RETRY_ENABLED', True)
    settings.set('RETRY_TIMES', 3)  
    settings.set('DOWNLOAD_TIMEOUT', 60)
    settings.set('DOWNLOAD_DELAY', 1)
    settings.set('LOG_FILE', LOG_FILE_PATH)
    settings.set('LOG_LEVEL', 'INFO') # logging level for Scrapy's internal
    
    set_playwright_settings(settings, spiders)

    process = CrawlerProcess(settings)
    process.settings.set('ITEM_PIPELINES', {'__main__.RunSpiderPipeline': 1})

    return process

def set_playwright_settings(settings, spiders):
    """
    If any spider has Playwright enabled, the function sets the Playwright 
    to run in headless mode (True).

    Args:
    settings (Settings): Scrapy settings object to configure.
    spiders (list): List of spider names (to check for Playwright support)
    """
    spider_loader = SpiderLoader.from_settings(settings)
    for spider_name in spiders:
        spider_cls = spider_loader.load(spider_name)
        if hasattr(spider_cls, 'playwright_enabled') and spider_cls.playwright_enabled:
            settings.set('PLAYWRIGHT_LAUNCH_OPTIONS', {"headless": True})
            logger.info(f"Playwright detected for spider '{spider_name}', forcing headless mode.")
            break

def run_specific_spiders():
    """
    Runs only the spiders specified in the SPECIFIC_SPIDERS list.

    The function resets global counters for success and failure, verifies 
    that the spiders exist in the project, and runs each spider from the SPECIFIC_SPIDERS list.

    Global Variables:
    - success_count (int): Counter for successful spiders.
    - fail_count (int): Counter for failed spiders.
    - success_spiders (list): List of successfully completed spiders.
    - fail_spiders (list): List of spiders that failed.
    """
    global success_count, fail_count, success_spiders, fail_spiders
    success_count, fail_count = 0, 0 # Reset the counters
    success_spiders, fail_spiders = [], [] # Reset lists

    spider_loader = SpiderLoader.from_settings(get_project_settings())
    all_spiders = spider_loader.list()

    setup_output_folder(output_folder, SPECIFIC_SPIDERS)

    process = setup_crawler(SPECIFIC_SPIDERS)

    for spider_name in SPECIFIC_SPIDERS:
        if spider_name in all_spiders:
            spider_cls = spider_loader.load(spider_name)
            process.crawl(spider_cls)
        else:
            logger.warning(f"Spider '{spider_name}' not found. Skipping...")

    process.start()

    logger.info(f"SUCCESSFUL: {success_count} spiders. Spiders: {success_spiders}")
    logger.info(f"FAILED: {fail_count} spiders. Spiders: {fail_spiders}")

def run_all_spiders():
    """
    Runs all spiders defined in the project (in 'spiders' folder).

    The function resets global counters for success and failure, loads all spiders from the project, 
    runs each one, and logs the results. It also ensures that the output folder is set up correctly 
    before running the spiders.

    Global Variables:
    - success_count (int): Counter for successful spiders.
    - fail_count (int): Counter for failed spiders.
    - success_spiders (list): List of successfully completed spiders.
    - fail_spiders (list): List of spiders that failed.
    """
    global success_count, fail_count, success_spiders, fail_spiders
    success_count, fail_count = 0, 0  # Reset the counters
    success_spiders, fail_spiders = [], []  # Reset lists

    spider_loader = SpiderLoader.from_settings(get_project_settings())
    all_spiders = spider_loader.list()

    setup_output_folder(output_folder, all_spiders)

    process = setup_crawler(all_spiders)

    for spider_name in all_spiders:
        spider_cls = spider_loader.load(spider_name)
        process.crawl(
            spider_cls,
            spider_name=spider_name,  # Meta data to identify failed spiders
        )

    process.start()

    logger.info(f"SUCCESSFUL: {success_count} spiders. Spiders: {success_spiders}")
    logger.info(f"FAILED: {fail_count} spiders. Spiders: {fail_spiders}")

class RunSpiderPipeline:
    """
    Pipeline to collect and store the results when a spider crawls website.

    Attributes:
        results (dict): Stores scraped items for each spider.

    Methods:
        open_spider(spider): Initializes results for the spider and logs its start.
        process_item(item, spider): Appends scraped items to the spider's results.
        close_spider(spider): Writes results to a JSON file if data was collected, or logs a failure.
    """

    def __init__(self):
        self.results = {}

    def open_spider(self, spider):
        self.results[spider.name] = []
        logger.info(f"Running spider '{spider.name}' ..")

    def process_item(self, item, spider):
        self.results[spider.name].append(item)
        return item

    def close_spider(self, spider):
        global success_count, fail_count, success_spiders, fail_spiders

        if self.results[spider.name]:
            output_file = os.path.join(output_folder, f"{spider.name}.json")
            with open(output_file, 'w') as f:
                f.write("[\n")
                for idx, result in enumerate(self.results[spider.name]):
                    json.dump(result, f)
                    if idx < len(self.results[spider.name]) - 1:
                        f.write(",\n")  # Add a comma after each object except the last
                    else:
                        f.write("\n")  # No comma after the last object
                f.write("]\n")
            logger.info(f"Done! spider '{spider.name}' data is written to {output_file}")
            success_count += 1
            success_spiders.append(spider.name)
        else:
            logger.warning(f"Spider '{spider.name}' failed to collect any data. No file was created.")
            fail_count += 1
            fail_spiders.append(spider.name)


if __name__ == "__main__":
    output_folder = "./data/spiders_output"
    backup_folder = "./backups"

    # Option 1: If running specific spiders
    backup_spider_outputs(output_folder, SPECIFIC_SPIDERS, backup_folder)
    run_specific_spiders()

    # Option 2: If running all spiders
    # spider_loader = SpiderLoader.from_settings(get_project_settings())
    # all_spiders = spider_loader.list()
    # backup_spider_outputs(output_folder, all_spiders, backup_folder)
    # run_all_spiders()

    filter_custom_logs()

