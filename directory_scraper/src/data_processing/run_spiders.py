import json
import os
import shutil
import logging
from datetime import datetime
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from scrapy.spiderloader import SpiderLoader
import re
from datetime import datetime
import argparse


# Logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # logging level for custom logger

LOG_DIR = os.path.join(os.path.dirname(__file__), '../..', 'logs') 
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)
LOG_FILE_NAME = 'run_spiders.log'
LOG_FILE_PATH = os.path.join(LOG_DIR, LOG_FILE_NAME)

file_handler = logging.FileHandler(LOG_FILE_PATH, mode='w')
file_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)

logger.addHandler(file_handler)
logger.propagate = True  # Propagate logs to Scrapy's root logger

# Global success and fail counters
success_count = 0
fail_count = 0
success_spiders = []
fail_spiders = []

def run_spiders(spider_list, output_folder, backup_folder):
    backup_spider_outputs(output_folder, spider_list, backup_folder)
    run_specific_spiders(spider_list, output_folder)

def filter_custom_logs(LOG_FILE_PATH=LOG_FILE_PATH):
    """
    This function filters out custom logs from the main log file (LOG_FILE_PATH),
    and writes only the entries generated by the custom logger (identified using the __name__ attribute) into a new log file, 
    excluding Scrapy's internal logs (identified using regex pattern) (e.g., [scrapy.core.engine], [scrapy-playwright], etc)

    Reason: Scrapy manages the entire logging system for spiders, making it hard to isolate custom logs from Scrapy's logs during runtime. 
    This function filters the logs post-run, allowing us to separate and extract only the custom logs based on the module's name (__name__).

    Arguments:
    LOG_FILE_PATH : str (optional)
    The path to the main log file to filter (defaults to LOG_FILE_PATH).
    """
    input_log = LOG_FILE_PATH
    output_log = LOG_FILE_PATH.replace('.log', '_custom.log')
    
    custom_logger_name = f"[{__name__}]"
    scrapy_pattern = re.compile(r"\[scrapy(?:\..*?)?\]")
    non_scrapy_pattern = re.compile(r", (DEBUG|INFO|WARNING|ERROR|CRITICAL) - .* - ")

    try:
        with open(input_log, "r") as infile:
            with open(output_log, "w") as outfile:
                for line in infile:
                    if scrapy_pattern.search(line):
                        continue
                    elif custom_logger_name in line:
                        outfile.write(line)
                    elif non_scrapy_pattern.search(line) and "scrapy" not in line:
                        outfile.write(line)
                    elif not scrapy_pattern.search(line) and "scrapy" not in line:
                        outfile.write(line)
                        
        print(f"Filtered custom logs have been written to {output_log}")
    except FileNotFoundError:
        print(f"Error: The file {input_log} does not exist.")
    except Exception as e:
        print(f"An error occurred: {e}")

def setup_output_folder(folder_path, spider_names):
    """
    - Prepares the output folder for storing spider results.
    - Creates the output folder if it doesn't exist.
    - Deletes any files or directories in the folder that match the spider names.

    Args:
    folder_path (str): The path to the folder where spider output will be stored.
    spider_names (list): List of spider names whose related files/folders need to be removed.
    """
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
    else:
        for f in os.listdir(folder_path):
            file_path = os.path.join(folder_path, f)
            file_name, file_ext = os.path.splitext(f)
            try:
                if file_name in spider_names and os.path.isfile(file_path):
                    os.remove(file_path)
                    logger.info(f"Deleted file: {file_path}")
                elif file_name in spider_names and os.path.isdir(file_path):
                    shutil.rmtree(file_path)
                    logger.info(f"Deleted directory: {file_path}")
            except Exception as e:
                logger.warning(f"Failed to delete {file_path}. Reason: {e}")
        logger.info(f"Done deleting relevant files in {folder_path}")

def backup_spider_outputs(output_folder, spider_names, backup_folder, max_backups=5):
    """
    Backs up existing spider output files to a backup folder.
    Creates timestamped backups and removes older backups if exceeding max_backups.

    Args:
    output_folder (str): Path to the folder containing the spider output files.
    spider_names (list): List of spider names whose output should be backed up.
    backup_folder (str): Path to the folder where backups will be stored.
    max_backups (int): Maximum number of backups to retain for each spider outputs. 
    """

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if not os.path.exists(backup_folder):
        os.makedirs(backup_folder)

    for spider_name in spider_names:
        spider_output_file = os.path.join(output_folder, f"{spider_name}.json")
        if os.path.exists(spider_output_file):
            spider_backup_folder = os.path.join(backup_folder, spider_name)
            if not os.path.exists(spider_backup_folder):
                os.makedirs(spider_backup_folder)
            
            backup_file_path = os.path.join(spider_backup_folder, f"{spider_name}_{timestamp}.json")
            try:
                shutil.copy(spider_output_file, backup_file_path)
                logger.info(f"Backed up {spider_output_file} to {backup_file_path}")
            except Exception as e:
                logger.error(f"Error backing up {spider_output_file}: {e}")

            backups = sorted(os.listdir(spider_backup_folder), reverse=True)
            if len(backups) > max_backups:
                backups_to_remove = backups[max_backups:]
                for backup in backups_to_remove:
                    os.remove(os.path.join(spider_backup_folder, backup))
                    logger.info(f"Removed old backup: {backup}")

def setup_crawler(spiders):
    """
    Sets up and configures the Scrapy crawler with custom settings.
    Configures retry attempts, timeouts, logging, and integrates custom pipelines.

    Args:
    spiders (list): List of spiders for crawling.

    Returns:
    CrawlerProcess: A Scrapy CrawlerProcess instance with configured settings.
    """
    settings = get_project_settings()
    settings.set('RETRY_ENABLED', True)
    settings.set('RETRY_TIMES', 3)  
    settings.set('DOWNLOAD_TIMEOUT', 60)
    settings.set('DOWNLOAD_DELAY', 1)
    settings.set('LOG_FILE', LOG_FILE_PATH)
    settings.set('LOG_LEVEL', 'INFO') # logging level for Scrapy's internal
    
    set_playwright_settings(settings, spiders)

    process = CrawlerProcess(settings)
    process.settings.set('ITEM_PIPELINES', {'__main__.RunSpiderPipeline': 1})

    return process

def set_playwright_settings(settings, spiders):
    """
    If any spider has Playwright enabled, the function sets the Playwright 
    to run in headless mode (True).

    Args:
    settings (Settings): Scrapy settings object to configure.
    spiders (list): List of spider names (to check for Playwright support)
    """
    spider_loader = SpiderLoader.from_settings(settings)
    for spider_name in spiders:
        spider_cls = spider_loader.load(spider_name)
        if hasattr(spider_cls, 'playwright_enabled') and spider_cls.playwright_enabled:
            settings.set('PLAYWRIGHT_LAUNCH_OPTIONS', {"headless": True})
            logger.info(f"Playwright detected for spider '{spider_name}', forcing headless mode.")
            break

def run_specific_spiders(SPECIFIC_SPIDERS, output_folder):
    """
    Runs only the spiders specified in the SPECIFIC_SPIDERS list.

    The function resets global counters for success and failure, verifies 
    that the spiders exist in the project, and runs each spider from the SPECIFIC_SPIDERS list.

    Global Variables:
    - success_count (int): Counter for successful spiders.
    - fail_count (int): Counter for failed spiders.
    - success_spiders (list): List of successfully completed spiders.
    - fail_spiders (list): List of spiders that failed.
    """
    global success_count, fail_count, success_spiders, fail_spiders
    success_count, fail_count = 0, 0 # Reset the counters
    success_spiders, fail_spiders = [], [] # Reset lists

    spider_loader = SpiderLoader.from_settings(get_project_settings())
    all_spiders = spider_loader.list()

    setup_output_folder(output_folder, SPECIFIC_SPIDERS)

    start_time = datetime.now()
    logger.info(f"Spider process started.")

    process = setup_crawler(SPECIFIC_SPIDERS)

    for spider_name in SPECIFIC_SPIDERS:
        if spider_name in all_spiders:
            spider_cls = spider_loader.load(spider_name)
            process.crawl(spider_cls)
        else:
            logger.warning(f"Spider '{spider_name}' not found. Skipping...")

    process.start()

    end_time = datetime.now()
    total_duration = end_time - start_time
    logger.info(f"Completed all spiders. Total run time: {total_duration}")

    logger.info(f"SUCCESSFUL: {success_count} spiders. Spiders: {success_spiders}")
    logger.info(f"FAILED: {fail_count} spiders. Spiders: {fail_spiders}")

def run_all_spiders(output_folder):
    """
    Runs all spiders defined in the project (in 'spiders' folder).

    The function resets global counters for success and failure, loads all spiders from the project, 
    runs each one, and logs the results. It also ensures that the output folder is set up correctly 
    before running the spiders.

    Global Variables:
    - success_count (int): Counter for successful spiders.
    - fail_count (int): Counter for failed spiders.
    - success_spiders (list): List of successfully completed spiders.
    - fail_spiders (list): List of spiders that failed.
    """
    global success_count, fail_count, success_spiders, fail_spiders
    success_count, fail_count = 0, 0  # Reset the counters
    success_spiders, fail_spiders = [], []  # Reset lists

    spider_loader = SpiderLoader.from_settings(get_project_settings())
    all_spiders = spider_loader.list()

    setup_output_folder(output_folder, all_spiders)

    start_time = datetime.now()
    logger.info(f"Spider process started at {start_time}")

    process = setup_crawler(all_spiders)

    for spider_name in all_spiders:
        spider_cls = spider_loader.load(spider_name)
        process.crawl(
            spider_cls,
            spider_name=spider_name,  # Meta data to identify failed spiders
        )

    process.start()

    end_time = datetime.now()
    total_duration = end_time - start_time
    logger.info(f"All spiders completed. Total run time: {total_duration}")

    logger.info(f"SUCCESSFUL: {success_count} spiders. Spiders: {success_spiders}")
    logger.info(f"FAILED: {fail_count} spiders. Spiders: {fail_spiders}")

class RunSpiderPipeline:
    """
    Pipeline to collect and store the results when a spider crawls website.

    Attributes:
        results (dict): Stores scraped items for each spider.

    Methods:
        open_spider(spider): Initializes results for the spider and logs its start.
        process_item(item, spider): Appends scraped items to the spider's results.
        close_spider(spider): Writes results to a JSON file if data was collected, or logs a failure.
    """

    def __init__(self):
        self.results = {}

    def open_spider(self, spider):
        self.start_time = datetime.now()
        self.results[spider.name] = []
        logger.info(f"Running spider '{spider.name}' ..")

    def process_item(self, item, spider):
        self.results[spider.name].append(item)
        return item

    def close_spider(self, spider):
        end_time = datetime.now()
        duration = end_time - self.start_time
        logger.info(f"Finished spider '{spider.name}'. Duration: {duration}")

        global success_count, fail_count, success_spiders, fail_spiders

        if self.results[spider.name]:
            output_file = os.path.join(output_folder, f"{spider.name}.json")
            with open(output_file, 'w') as f:
                f.write("[\n")
                for idx, result in enumerate(self.results[spider.name]):
                    json.dump(result, f)
                    if idx < len(self.results[spider.name]) - 1:
                        f.write(",\n")  # Add a comma after each object except the last
                    else:
                        f.write("\n")  # No comma after the last object
                f.write("]\n")
            logger.info(f"Successfully written '{spider.name}' data to {output_file}")
            success_count += 1
            success_spiders.append(spider.name)
        else:
            logger.warning(f"Spider '{spider.name}' failed to collect any data. No file was created.")
            fail_count += 1
            fail_spiders.append(spider.name)

def main():
    parser = argparse.ArgumentParser(
        description=(
            "Run Scrapy spiders. Available spider_selection are:\n"
            "\n"
            "1. 'all' - to run all spiders in the project.\n"
            "2. 'listed' - to run a predefined list of spiders (from the SPECIFIC_SPIDERS list).\n"
            "3. A single spider name - to run a specific spider (e.g., 'jpm').\n"
            "4. A comma-separated list of spiders - to run multiple specific spiders (e.g., 'jpm,mof,miti').\n"
            "\n"
            "E.g:\n"
            "python run_spiders all\n"
            "python run_spiders listed\n"
            "python run_spiders jpm\n"
            "python run_spiders jpm,mof,miti\n"
            "\n"
            "Ensure the spider names are valid, or the program will return an error."

        ),
            formatter_class=argparse.RawTextHelpFormatter  # preserve formatting

    )

    parser.add_argument(
        "spider_selection", 
        help=(
            "spider_selection: 'all' to run all spiders, 'listed' for predefined spiders, "
            "or specify one or more valid spider names (separated by commas)."
        )
    )
    args = parser.parse_args()

    output_folder = "./data/spiders_output"
    backup_folder = "./backups"

# =============================================================================
# ======================= SPIDER_SELECTION OPTIONS ============================
# =============================================================================

    spider_loader = SpiderLoader.from_settings(get_project_settings())
    all_spiders = spider_loader.list()

    SPECIFIC_SPIDERS = [
        "jpm",
        "mof",
        # "rurallink_anggota",
        # "rurallink_pkd",
        # "petra",
        # "mot",
        # "kpkm",
        # "ekonomi",
        # "kpkt",
        # "kln",
        # "kkr",
        # "moha",
        # "miti",
        # "mod",
        # "mosti",
        # "kpwkm",
        # "nres",
        # "kuskop",
        # "kpt",
        # "motac",
        # "komunikasi",
        # "moe",
        # "kpn",
        # "kbs",
        # "kpdn",
        # "kpk",
        # "digital",
        # "moh",
        # "mohr"
    ]

    # Option 1: run all spiders in the project
    if args.spider_selection == "all":
        backup_spider_outputs(output_folder, all_spiders, backup_folder)
        run_all_spiders(output_folder)

    # Option 2: run list of specific spiders based on SPECIFIC_SPIDERS
    elif args.spider_selection == "listed":
        if SPECIFIC_SPIDERS:
            backup_spider_outputs(output_folder, SPECIFIC_SPIDERS, backup_folder)
            run_spiders(SPECIFIC_SPIDERS, output_folder, backup_folder)
        else:
            logger.error("No spiders listed in SPECIFIC_SPIDERS.")

    # Option 3: run list specific spiders on command line (separated by comma)
    elif "," in args.spider_selection:
        spider_list = [spider.strip() for spider in args.spider_selection.split(",")]
        valid_spiders = [spider for spider in spider_list if spider in all_spiders]
        invalid_spiders = [spider for spider in spider_list if spider not in all_spiders]

        if valid_spiders:
            backup_spider_outputs(output_folder, valid_spiders, backup_folder)
            run_spiders(valid_spiders, output_folder, backup_folder)
        else:
            logger.error(f"No valid spiders found in list: {spider_list}")

        if invalid_spiders:
            logger.warning(f"The following spiders were not found: {invalid_spiders}")

    # Option 4: run only 1 specific spider
    elif args.spider_selection in all_spiders:
        backup_spider_outputs(output_folder, [args.spider_selection], backup_folder)
        run_spiders([args.spider_selection], output_folder, backup_folder)

    # Do nothing if no valid spider_selection is provided
    else:
        logger.error(f"Invalid command or spider name: '{args.spider_selection}'. Please provide a valid spider_selection, or check the spider name.")
        logger.info("Available spider_selection(s): 'all', 'listed', a spider name (e.g 'jpm'), or a list of spiders names (e.g 'jpm,miti,mof')")
        logger.info(f"Available spiders names are: {all_spiders}")

# =============================================================================
# ========================== END OF SELECTION =================================
# =============================================================================

    # Custom logging. Run this once after all the spiders have finished
    filter_custom_logs()

if __name__ == "__main__":
    main()